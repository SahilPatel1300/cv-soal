{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Directory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get File Names and Categories and Data Amount\n",
    "# File exploration\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "def analyze_directory(path):\n",
    "    file_pattern = re.compile(r\"^(.*?)(\\d{5})\\..+$\")  # captures category and 5-digit number\n",
    "    category_files = defaultdict(list)\n",
    "\n",
    "    for filename in os.listdir(path):\n",
    "        match = file_pattern.match(filename)\n",
    "        if match:\n",
    "            category, number_str = match.groups()\n",
    "            category_files[category].append((filename, int(number_str)))\n",
    "\n",
    "    # Print summary of categories\n",
    "    for category, files in category_files.items():\n",
    "        numbers = [num for _, num in files]\n",
    "        print(f\"Category: {category}\")\n",
    "        print(f\"  Number of files: {len(files)}\")\n",
    "        print(f\"  Number range: {min(numbers)} to {max(numbers)}\")\n",
    "\n",
    "    print(\"\\nInspecting one file per category:\")\n",
    "    for category, files in category_files.items():\n",
    "        sample_file = next(f for f in files if f[0].endswith('.npz'))[0]\n",
    "        filepath = os.path.join(path, sample_file)\n",
    "        print(f\"\\nSample file for category '{category}': {sample_file}\")\n",
    "        try:\n",
    "            data = np.load(filepath)\n",
    "            for key in data:\n",
    "                print(f\"  Key: {key}, Shape: {data[key].shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Could not load file '{sample_file}': {e}\")\n",
    "\n",
    "# Example usage:\n",
    "analyze_directory(\"../dexnet_2.1/dexnet_2.1_eps_10/tensors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Define all categories and their shape descriptions\n",
    "categories = {\n",
    "    'camera_poses_': (1000, 7),\n",
    "    'hand_poses_': (1000, 6),\n",
    "    'depth_ims_tf_table_': (466, 32, 32, 1),\n",
    "    'labels_': (1000,),\n",
    "    'traj_ids_': (1000,),\n",
    "    'grasp_metrics_': (1000,),\n",
    "    'camera_intrs_': (1000, 4),\n",
    "    'grasped_obj_keys_': (1000,),\n",
    "    'grasp_collision_metrics_': (1000,),\n",
    "    'pile_ids_': (1000,)\n",
    "}\n",
    "\n",
    "def load_file(path, category, file_num):\n",
    "    fname = f\"{category}{file_num:05d}.npz\"\n",
    "    fpath = os.path.join(path, fname)\n",
    "    return np.load(fpath)['arr_0']\n",
    "\n",
    "def find_common_file_numbers(path):\n",
    "    files = os.listdir(path)\n",
    "    category_to_nums = {cat: set() for cat in categories}\n",
    "    for fname in files:\n",
    "        for cat in categories:\n",
    "            if fname.startswith(cat) and fname.endswith('.npz'):\n",
    "                try:\n",
    "                    num = int(fname[len(cat):-4])\n",
    "                    category_to_nums[cat].add(num)\n",
    "                except:\n",
    "                    continue\n",
    "    # Find intersection of all sets\n",
    "    common_nums = set.intersection(*category_to_nums.values())\n",
    "    return sorted(list(common_nums))\n",
    "\n",
    "def visualize_random_example(path):\n",
    "    common_files = find_common_file_numbers(path)\n",
    "    if not common_files:\n",
    "        print(\"No common file numbers found across all categories.\")\n",
    "        return\n",
    "\n",
    "    chosen_file_num = random.choice(common_files)\n",
    "\n",
    "    # Load a sample file to determine valid index range\n",
    "    depth_map = load_file(path, 'depth_ims_tf_table_', chosen_file_num)\n",
    "    max_index = depth_map.shape[0]  # Likely 466\n",
    "    chosen_index = random.randint(0, max_index - 1)\n",
    "\n",
    "    print(f\"Selected file number: {chosen_file_num:05d}, sample index: {chosen_index}\\n\")\n",
    "\n",
    "    # Store and print/plot each category\n",
    "    for category in categories:\n",
    "        data = load_file(path, category, chosen_file_num)\n",
    "\n",
    "        if category == 'depth_ims_tf_table_':\n",
    "            image = data[chosen_index].squeeze()\n",
    "            plt.figure()\n",
    "            plt.title(\"Depth Map\")\n",
    "            plt.imshow(image, cmap='gray')\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "\n",
    "        elif category == 'grasp_metrics_':\n",
    "            plt.figure()\n",
    "            plt.title(\"Grasp Metric (value)\")\n",
    "            plt.bar([0], [data[chosen_index]])\n",
    "            plt.xticks([0], ['Grasp Metric'])\n",
    "            plt.ylabel('Score')\n",
    "            plt.show()\n",
    "\n",
    "        else:\n",
    "            print(f\"{category}{chosen_file_num:05d} -> Example[{chosen_index}]: {data[chosen_index]}\\n\")\n",
    "\n",
    "# Example usage\n",
    "# Replace this path with the actual path to your data folder\n",
    "visualize_random_example(\"../dexnet_2.1/dexnet_2.1_eps_10/tensors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd scripts/\n",
    "# ./download_dexnet_2.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['image']['depth_ims']     # Shape: (N, 32, 32)   — depth image\n",
    "# dataset['pose']                   # Shape: (N, 4)        — grasp pose (x, y, z, angle)\n",
    "# dataset['success']                # Shape: (N,)          — binary label: success/failure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For tensorfloaw dataset\n",
    "\"\"\"\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset\n",
    "# import h5py\n",
    "# import numpy as np\n",
    "\n",
    "# class DexNetDataset(Dataset):\n",
    "#     def __init__(self, h5_path):\n",
    "#         self.data = h5py.File(h5_path, 'r')\n",
    "#         self.depth_images = self.data['image']['depth_ims'][:]\n",
    "#         self.labels = self.data['grasp_qualities'][:]  # or 'success', depending on file\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.depth_images)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img = self.depth_images[idx]\n",
    "#         img = np.expand_dims(img, axis=0)  # Convert to (1, H, W) for PyTorch CNN\n",
    "#         label = self.labels[idx]\n",
    "#         return torch.tensor(img, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# dataset = DexNetDataset('path/to/dexnet_dataset.h5')\n",
    "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from customize_dataset import DexNetNPZDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = DexNetNPZDataset('../dexnet_2.1/dexnet_2.1_eps_10/tensors/')\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset with regression labels and full 6D poses\n",
    "from customize_dataset import DexNetNPZDatasetAll\n",
    "from torch.utils.data import DataLoader\n",
    "dataset = DexNetNPZDatasetAll(tensor_dir='../dexnet_2.1/dexnet_2.1_eps_10/tensors/', use_regression=True, pose_dims=[0, 1, 2, 3, 4, 5])\n",
    "img, pose, label = dataset[0]\n",
    "print(img.shape)    # torch.Size([3, 32, 32])\n",
    "print(pose.shape)   # torch.Size([6])\n",
    "print(label)        # A float (grasp quality)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dataloader:\n",
    "    print(i[0].shape, i[1])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3)\n",
    "        self.fc1 = nn.Linear(32 * 6 * 6, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)  # Binary classification (grasp success)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # (B, 16, 15, 15)\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # (B, 32, 6, 6)\n",
    "        x = x.view(-1, 32 * 6 * 6)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "class ResNetGraspNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A binary classification model for grasp success using ResNet-50 as a backbone.\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained: bool = True, num_classes: int = 1, dropout_rate: float = 0.2):\n",
    "        super(ResNetGraspNet, self).__init__()\n",
    "        # Load ResNet-50 backbone with optional ImageNet pretrained weights\n",
    "        weights = ResNet50_Weights.IMAGENET1K_V2 if pretrained else None\n",
    "        self.backbone = resnet50(weights=weights)\n",
    "        # Retrieve number of input features to the classifier head\n",
    "        in_features = self.backbone.fc.in_features\n",
    "        # Replace the original classification head\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate, inplace=True),\n",
    "            nn.Linear(in_features, num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Forward pass through ResNet-50 and sigmoid head\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights\n",
    "\n",
    "class EfficientNetV2GraspNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A binary classification model for grasp success using EfficientNet-V2-S as a backbone.\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained: bool = True, num_classes: int = 1, dropout_rate: float = 0.2):\n",
    "        super(EfficientNetV2GraspNet, self).__init__()\n",
    "        # Load EfficientNet-V2-S backbone with optional ImageNet pretrained weights\n",
    "        weights = EfficientNet_V2_S_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "        self.backbone = efficientnet_v2_s(weights=weights)\n",
    "        # Retrieve number of input features to the classifier head\n",
    "        in_features = self.backbone.classifier[1].in_features\n",
    "        # Replace the classifier head for binary grasp success prediction\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate, inplace=True),\n",
    "            nn.Linear(in_features, num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Forward pass through EfficientNet-V2 and sigmoid head\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass and Evaluation Measurements Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in model.parameters():\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SimpleCNN()\n",
    "model = EfficientNetV2GraspNet()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "from tqdm import tqdm\n",
    "for epoch in range(3):\n",
    "    for imgs, labels in tqdm(dataloader):\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 3\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for imgs, labels in progress_bar:\n",
    "        outputs = model(imgs).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate metrics\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "        # Optional: compute accuracy\n",
    "        predicted = (outputs >= 0.5).float()\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        break\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    accuracy = correct / total\n",
    "\n",
    "    loss_history.append(epoch_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] — Loss: {epoch_loss:.4f} — Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_history, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss per Epoch')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image + Poses Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleGQCNN(nn.Module):\n",
    "    def __init__(self, pose_dim=4, output_type='binary', merge_methods=\"element_dot\"):\n",
    "        \"\"\"\n",
    "        pose_dim: number of dimensions in the pose vector (e.g., x, y, z, theta)\n",
    "        output_type: 'binary' or 'regression'\n",
    "        \"\"\"\n",
    "        super(SimpleGQCNN, self).__init__()\n",
    "        self.output_type = output_type\n",
    "\n",
    "        # Image stream\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3)           # → (B, 16, 30, 30)\n",
    "        self.pool = nn.MaxPool2d(2, 2)             # → (B, 16, 15, 15)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3)          # → (B, 32, 13, 13) → pool → (B, 32, 6, 6)\n",
    "        self.im_fc = nn.Linear(32 * 6 * 6, 64)     # → (B, 64)\n",
    "\n",
    "        # Pose stream\n",
    "        self.pose_fc1 = nn.Linear(pose_dim, 64)\n",
    "        self.pose_fc2 = nn.Linear(64, 64)\n",
    "\n",
    "        self.merge_methods = merge_methods\n",
    "        if self.merge_methods == \"element_dot\":\n",
    "            # Merge stream after elementwise multiplication\n",
    "            self.merge_fc1 = nn.Linear(64, 32)\n",
    "            self.merge_fc2 = nn.Linear(32, 1)\n",
    "        else:\n",
    "            # Merge stream by concatanation\n",
    "            self.merge_fc1 = nn.Linear(64 + 64, 64)\n",
    "            self.merge_fc2 = nn.Linear(64, 1)  # Single output for binary or regression\n",
    "\n",
    "    def forward(self, image, pose):\n",
    "        \n",
    "        # Image stream\n",
    "        x = self.pool(F.relu(self.conv1(image)))   # (B, 16, 15, 15)\n",
    "        x = self.pool(F.relu(self.conv2(x)))       # (B, 32, 6, 6)\n",
    "        x = x.view(x.size(0), -1)                  # Flatten\n",
    "        x = F.relu(self.im_fc(x))                  # (B, 64)\n",
    "\n",
    "        # Pose stream\n",
    "        p = F.relu(self.pose_fc1(pose))            # (B, 64)\n",
    "        p = F.relu(self.pose_fc2(p))               # (B, 64)\n",
    "\n",
    "        if self.merge_methods == \"element_dot\":\n",
    "            # Element-wise multiplication\n",
    "            combined = x * p                           # (B, 64)\n",
    "        else:\n",
    "            # Merge\n",
    "            combined = torch.cat((x, p), dim=1)       # -> (B, 96)\n",
    "\n",
    "        # Final layers\n",
    "        out = F.relu(self.merge_fc1(combined))     # (B, 32)\n",
    "        out = self.merge_fc2(out)                  # (B, 1)\n",
    "\n",
    "        if self.output_type == 'binary':\n",
    "            out = torch.sigmoid(out)               # Binary prediction\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleGQCNN(pose_dim=4, output_type='regression')  # or 'binary'\n",
    "\n",
    "image = torch.randn(8, 1, 32, 32)  # Batch of 8 grayscale images\n",
    "pose = torch.randn(8, 4)          # Corresponding batch of 4D poses\n",
    "\n",
    "output = model(image, pose)\n",
    "print(output.shape)  # torch.Size([8, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "# Set config\n",
    "tensor_dir = '../dexnet_2.1/dexnet_2.1_eps_10/tensors/'  # replace with actual path\n",
    "batch_size = 32\n",
    "use_regression = False  # or True\n",
    "pose_dims = [2]          # or full [0, 1, 2, 3, 4, 5]\n",
    "torch.backends.cudnn.benchmark = True\n",
    "# Create dataset and dataloader\n",
    "\n",
    "\n",
    "dataset = DexNetNPZDatasetAll(tensor_dir=tensor_dir, use_regression=use_regression, pose_dims=pose_dims)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "model = SimpleGQCNN(pose_dim=len(pose_dims), output_type='regression' if use_regression else 'binary')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "if use_regression:\n",
    "    criterion = nn.MSELoss()\n",
    "else:\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, scheduler, device, num_epochs=10, save_path=\"model.pth\"):\n",
    "    model = model.to(device, memory_format=torch.channels_last)\n",
    "    model.train()\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    loss_history = []\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for images, poses, labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            images = images.to(device, non_blocking=True).contiguous(memory_format=torch.channels_last)\n",
    "            poses = poses.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True).unsqueeze(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                outputs = model(images, poses)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(dataloader)\n",
    "        loss_history.append(avg_loss)\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, LR: {current_lr:.2e}\")\n",
    "\n",
    "        # Save model if it's the best so far\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"✅ Saved best model with loss {best_loss:.4f} to '{save_path}'\")\n",
    "\n",
    "    # Visualize training loss\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(range(1, num_epochs + 1), loss_history, marker='o')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss Over Epochs\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"loss_plot.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, dataloader, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
