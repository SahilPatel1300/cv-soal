{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8d2af01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# setup cell to make our lives easier \n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from utils import *\n",
    "from eval_script import *\n",
    "from customize_dataset import DexNetNPZDataset\n",
    "from customize_dataset import DexNetNPZDatasetAll\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "tensor_dir = '../../dexnet_2.1/dexnet_2.1_eps_50/tensors/'  # replace with actual path\n",
    "batch_size = 32\n",
    "use_regression = False  # or True\n",
    "pose_dims = [0, 1, 2, 3, 4, 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da4b14cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup block. Run me!\n",
    "#print(torch.cuda.device_count())\n",
    "#print(torch.cuda.get_device_name())\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "727f20cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGQCNN(nn.Module):\n",
    "    def __init__(self, pose_dim=4, output_type='binary', merge_methods=\"element_dot\"):\n",
    "        \"\"\"\n",
    "        pose_dim: number of dimensions in the pose vector (e.g., x, y, z, theta)\n",
    "        output_type: 'binary' or 'regression'\n",
    "        \"\"\"\n",
    "        super(SimpleGQCNN, self).__init__()\n",
    "        self.output_type = output_type\n",
    "\n",
    "        # Image stream\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3)           # → (B, 16, 30, 30)\n",
    "        self.pool = nn.MaxPool2d(2, 2)             # → (B, 16, 15, 15)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3)          # → (B, 32, 13, 13) → pool → (B, 32, 6, 6)\n",
    "        self.im_fc = nn.Linear(32 * 6 * 6, 64)     # → (B, 64)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.im_fc_bn = nn.BatchNorm1d(64)\n",
    "\n",
    "        # Pose stream\n",
    "        self.pose_fc1 = nn.Linear(pose_dim, 64)\n",
    "        self.pose_fc2 = nn.Linear(64, 64)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.merge_methods = merge_methods\n",
    "        if self.merge_methods == \"element_dot\":\n",
    "            # Merge stream after elementwise multiplication\n",
    "            self.merge_fc1 = nn.Linear(64, 32)\n",
    "            self.merge_fc2 = nn.Linear(32, 1)\n",
    "        else:\n",
    "            # Merge stream by concatanation\n",
    "            self.merge_fc1 = nn.Linear(64 + 64, 64)\n",
    "            self.merge_fc2 = nn.Linear(64, 1)  # Single output for binary or regression\n",
    "\n",
    "    def forward(self, image, pose):\n",
    "        \n",
    "        # Image stream\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(image))))   # (B, 16, 15, 15)\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))       # (B, 32, 6, 6)\n",
    "        # x = x.view(x.size(0), -1)                  # Flatten\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.im_fc(x))                  # (B, 64)\n",
    "\n",
    "        # Pose stream\n",
    "        p = self.dropout(F.relu(self.pose_fc1(pose)))            # (B, 64)\n",
    "        p = self.dropout(F.relu(self.pose_fc2(p)))               # (B, 64)\n",
    "\n",
    "        if self.merge_methods == \"element_dot\":\n",
    "            # Element-wise multiplication\n",
    "            combined = x * p                           # (B, 64)\n",
    "        else:\n",
    "            # Merge\n",
    "            combined = torch.cat((x, p), dim=1)       # -> (B, 96)\n",
    "\n",
    "        # Final layers\n",
    "        out = F.relu(self.merge_fc1(combined))     # (B, 32)\n",
    "        out = self.merge_fc2(out)                  # (B, 1)\n",
    "\n",
    "        # if self.output_type == 'binary':\n",
    "        #     out = torch.sigmoid(out)               # Binary prediction\n",
    "        return out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11d25ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SimpleGQCNN\n",
    "pose_dims = [0, 1, 2, 3, 4, 5]\n",
    "use_regression = False  # or True\n",
    "\n",
    "model = SimpleGQCNN(pose_dim=len(pose_dims), output_type='regression' if use_regression else 'binary')\n",
    "model.load_state_dict(torch.load(\"eps_10/model.pth\", weights_only=False, map_location=torch.device('cpu')))\n",
    "model = model.to(device, memory_format=torch.channels_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd2d8bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Found 35 tensor files\n",
      "\n",
      "Processing 5 files...\n",
      "\n",
      "Processing file 1/5...\n",
      "File contains 1000 samples\n",
      "Running inference on 32 batches...\n",
      "Processed 1000 samples from file 1\n",
      "\n",
      "Processing file 2/5...\n",
      "File contains 1000 samples\n",
      "Running inference on 32 batches...\n",
      "Processed 1000 samples from file 2\n",
      "\n",
      "Processing file 3/5...\n",
      "File contains 1000 samples\n",
      "Running inference on 32 batches...\n",
      "Processed 1000 samples from file 3\n",
      "\n",
      "Processing file 4/5...\n",
      "File contains 1000 samples\n",
      "Running inference on 32 batches...\n",
      "Processed 1000 samples from file 4\n",
      "\n",
      "Processing file 5/5...\n",
      "File contains 1000 samples\n",
      "Running inference on 32 batches...\n",
      "Processed 1000 samples from file 5\n",
      "\n",
      "Model evaluation complete! Results saved to 'torch_evaluation_results.txt'\n",
      "Visualizations saved to directory: './eps_10'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.00186193, 0.71879286, 0.73538333, ..., 0.9605993 , 0.86230177,\n",
       "        0.9345029 ], shape=(4080,), dtype=float32),\n",
       " array([0, 1, 0, ..., 1, 1, 1], shape=(4080,)),\n",
       " array([0.00111575, 0.00238624, 0.00428153, ..., 0.00401029, 0.00233087,\n",
       "        0.00294281], shape=(4080,)))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_dir = \"../../dexnet_2.1/dexnet_2.1_eps_50/tensors/\"\n",
    "pose_dims = [0, 1, 2, 3, 4, 5]\n",
    "run_model_evaluation(\n",
    "    model=model,\n",
    "    tensor_dir=tensor_dir,\n",
    "    batch_size=batch_size,\n",
    "    pose_dims=pose_dims,\n",
    "    visualizations_dir=\"./eps_10\",\n",
    "    use_regression=use_regression,\n",
    "    num_files=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7936a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 85537\n",
      "Number of total parameters: 85537\n"
     ]
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {pytorch_total_params}\")\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of total parameters: {pytorch_total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f063d09f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_accuracy_with_confusion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m train_loader, val_loader \u001b[38;5;241m=\u001b[39m DexNetDataloader(tensor_dir\u001b[38;5;241m=\u001b[39mtensor_dir, use_regression\u001b[38;5;241m=\u001b[39muse_regression, pose_dims\u001b[38;5;241m=\u001b[39mpose_dims)\n\u001b[0;32m      5\u001b[0m subset_loader \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mislice(val_loader, \u001b[38;5;241m5\u001b[39m)  \u001b[38;5;66;03m# Use only the first 5 batches\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mevaluate_accuracy_with_confusion\u001b[49m(model, subset_loader, device, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'evaluate_accuracy_with_confusion' is not defined"
     ]
    }
   ],
   "source": [
    "# import itertools\n",
    "# # Create dataset and dataloader\n",
    "# train_loader, val_loader = DexNetDataloader(tensor_dir=tensor_dir, use_regression=use_regression, pose_dims=pose_dims)\n",
    " \n",
    "# subset_loader = itertools.islice(val_loader, 5)  # Use only the first 5 batches\n",
    "\n",
    "# evaluate_accuracy_with_confusion(model, subset_loader, device, threshold=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
